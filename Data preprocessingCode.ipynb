{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# afficher une partie du tableau \n",
    "dataset = pd.read_csv(\"C:\\\\Users\\\\boukh\\\\Downloads\\\\nouveau_fichier.csv\")\n",
    "#.head(): C'est une méthode de Pandas qui renvoie les premières lignes du DataFrame. Par défaut, elle renvoie les cinq premières lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fournir des informations détaillées sur le DataFrame(Le nombre total d'entrées non nulles dans chaque colonne ; le type de données de chaque colonne)\n",
    "dataset.info()\n",
    "# voir le nbr des informations manquantes dans chaque colonne\n",
    "#.isnull().sum() est utilisée pour compter le nombre de valeurs nulles (ou manquantes) dans chaque colonne du DataFrame\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.describe():génère un résumé statistique descriptif pour chaque colonne DataFrame.\n",
    "#Nombre d'éléments non manquants (count); Moyenne (mean);Écart type (std);Valeur minimale (min);Premier quartile (25%);Médiane (50% ou la valeur du deuxième quartile);Troisième quartile (75%);Valeur maximale (max)\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supprimer les colonnes vides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:\\\\Users\\\\boukh\\\\Downloads\\\\S4\\\\Projet pluridiciplinaire\\\\final .csv\")\n",
    "dataset.drop(columns=['specs_energie'], inplace=True)\n",
    "dataset.drop(columns=['specs_boite'], inplace=True)\n",
    "dataset.to_csv(\"C:\\\\Users\\\\boukh\\\\Downloads\\\\S4\\\\Projet pluridiciplinaire\\\\final .csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the total number of Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns\n",
    "cat_col = [col for col in dataset.columns if dataset[col].dtype == 'object']\n",
    "print('Categorical columns :',cat_col)\n",
    "# Numerical columns\n",
    "num_col = [col for col in dataset.columns if dataset[col].dtype != 'object']\n",
    "print('Numerical columns :',num_col)\n",
    "#the total number of Unique Values\n",
    "dataset[num_col].nunique()\n",
    "dataset[cat_col].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder les types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage one-hot des variables catégorielles\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('final.csv')\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['specs_energie'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['specs_couleur_auto '])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['specs_papiers '])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['specs_car_engine '])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['specs_marque-voiture'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['specs_modele'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['specs_finition'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['specs_boite'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option0'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option1'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option2'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option3'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option4'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option5'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option6'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option7'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option8'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option9'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option10'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option13'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option14'])\n",
    "dataset_encoded = pd.get_dummies(dataset, columns=['option15'])\n",
    "dataset_encoded.to_csv('final.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modifier les valeurs aberrantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner la colonne où vous voulez remplacer les valeurs aberrantes\n",
    "colonne_cible = 'nom_de_la_colonne'\n",
    "#........................................\n",
    "\n",
    "# Identifier les valeurs aberrantes dans la colonne cible\n",
    "ecart_type = 3  \n",
    "#.....................................\n",
    "moyenne = dataset[colonne_cible].mean()\n",
    "ecart_type = dataset[colonne_cible].std()\n",
    "seuil_superieur = moyenne + (ecart_type * ecart_type)\n",
    "seuil_inferieur = moyenne - (ecart_type * ecart_type)\n",
    "valeurs_aberrantes = dataset[(dataset[colonne_cible] > seuil_superieur) | (publish_display_data[colonne_cible] < seuil_inferieur)]\n",
    "\n",
    "# Remplacer les valeurs aberrantes par la médiane de la colonne\n",
    "median_value = dataset[colonne_cible].median()\n",
    "dataset.loc[valeurs_aberrantes.index, colonne_cible] = median_value\n",
    "\n",
    "dataset.to_csv('nouveau_fichier.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la corrélation entre les différentes variables du DataFrame.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exclure la colonne 'id' du calcul de la corrélation\n",
    "numeric_columns = dataset.drop(columns=['id']).select_dtypes(include=[np.number])\n",
    "corr = numeric_columns.corr()\n",
    "\n",
    "# Afficher le heatmap de corrélation\n",
    "plt.figure(dpi=130)\n",
    "sns.heatmap(corr, annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "#Les valeurs de corrélation près de 1 ou -1 indiquent une forte corrélation positive ou négative respectivement, tandis que les valeurs proches de 0 indiquent une faible corrélation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supprimes les doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes en double\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "# Réinitialiser l'index après la suppression des lignes en double\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "# Réécrire le DataFrame dans un nouveau fichier CSV si nécessaire\n",
    "dataset.to_csv('final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supprimer une colonne donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer la colonne spécifiée\n",
    "supp = 'Nom_de_la_colonne'\n",
    "dataset.drop(columns=[supp], inplace=True)\n",
    "\n",
    "# Enregistrer le DataFrame modifié dans un nouveau fichier CSV\n",
    "dataset.to_csv('nouveau_fichier.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remplacer les valeurs manquantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calucler la moyenne de la colonne \n",
    "mean_value = dataset['specs_finition'].mean()\n",
    "# Remplace les valeurs manquantes par la moyenne de la colonne\n",
    "dataset['specs_finition'].fillna(mean_value, inplace=True) \n",
    "dataset.to_csv('final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supprimer les lignes avec un prix nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les lignes avec un prix différent de zéro\n",
    "dataset = dataset[dataset['price']!=0 ]\n",
    "# Filtrer les lignes avec un prix inférieur à 10\n",
    "dataset= dataset[dataset['price'] >= 20]\n",
    "\n",
    "# Réinitialiser l'index après la suppression des lignes\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Réécrire le DataFrame dans un nouveau fichier CSV si nécessaire\n",
    "dataset.to_csv('final', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble des prix obtenus après filtrage :\n",
      "[3350000. 1230000. 3950000. ... 1205000. 5960000.   25500.]\n"
     ]
    }
   ],
   "source": [
    "ensemble_prix = dataset['price'].unique()\n",
    "\n",
    "# Afficher l'ensemble des prix\n",
    "print(\"Ensemble des prix obtenus après filtrage :\")\n",
    "print(ensemble_prix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indice(s) de(s) ligne(s) avec le prix recherché:\n",
      "80637\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prix_recherche = 25500  # Modifier selon le prix que vous recherchez\n",
    "\n",
    "# Rechercher les lignes avec le prix donné\n",
    "indices_lignes = dataset.index[dataset['price'] == prix_recherche].tolist()\n",
    "\n",
    "# Afficher les indices des lignes avec le prix donné\n",
    "if indices_lignes:\n",
    "    print(\"Indice(s) de(s) ligne(s) avec le prix recherché:\")\n",
    "    for indice in indices_lignes:\n",
    "        print(indice)\n",
    "else:\n",
    "    print(\"Aucune ligne avec le prix recherché n'a été trouvée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Indice de la ligne que vous souhaitez supprimer\n",
    "indice_a_supprimer = 80637\n",
    "  # Modifier avec l'indice de la ligne que vous souhaitez supprimer\n",
    "\n",
    "# Supprimer la ligne à l'indice donné\n",
    "dataset.drop(index=indice_a_supprimer, inplace=True)\n",
    "\n",
    "dataset.to_csv('final', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supprimer les lignes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes avec des valeurs manquantes dans la colonne \"Prix\"\n",
    "dataset.dropna(subset=['Prix'], inplace=True)\n",
    "# Réinitialiser l'index après la suppression des lignes\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "# Réécrire le DataFrame dans un nouveau fichier CSV si nécessaire\n",
    "dataset.to_csv('nouveau_fichier.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate independent features and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate array into input and output components\n",
    "X = dataset.drop(columns =['prix'])\n",
    "# la dataset x contient toutes les colonnes sauf la colonne prix\n",
    "Y = dataset.prix\n",
    "#la dataset y contient tuniquement la colonne prix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
